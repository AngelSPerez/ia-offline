# ia-offline üåê

Este repositorio aloja un proyecto de **Inteligencia Artificial sin conexi√≥n a Internet**, aprovechando principalmente las capacidades de **wllama**. El objetivo es proporcionar una experiencia de IA que funcione completamente sin necesidad de una conexi√≥n a Internet.

## üèÜ Insignias

[![Estrellas de GitHub](https://img.shields.io/github/stars/AngelSPerez/ia-offline?style=flat-square&logo=github)](https://github.com/AngelSPerez/ia-offline/stargazers)
[![Bifurcaciones de GitHub](https://img.shields.io/github/forks/AngelSPerez/ia-offline?style=flat-square&logo=github)](https://github.com/AngelSPerez/ia-offline/forks)
[![Licencia](https://img.shields.io/github/license/AngelSPerez/ia-offline?style=flat-square)](LICENSE)

## ‚ú® Caracter√≠sticas Principales

*   **Inteligencia Artificial Offline:** Ejecuta modelos de IA localmente, sin depender de una conexi√≥n a Internet.
*   **Basado en wllama:** Utiliza la potencia de wllama para el procesamiento de modelos de lenguaje.
*   **Experiencia de Usuario Fluida:** Dise√±ado para una interacci√≥n intuitiva y sin interrupciones.
*   **Instalaci√≥n Sencilla:** Procedimientos claros para poner en marcha el proyecto r√°pidamente.
*   **Personalizable:** Posibilidad de adaptar la interfaz y el comportamiento a necesidades espec√≠ficas.

## üöÄ Instalaci√≥n

Para instalar y ejecutar `ia-offline` localmente, siga estos pasos:

1.  **Clonar el Repositorio:**
    ```bash
    git clone https://github.com/AngelSPerez/ia-offline.git
    cd ia-offline
    ```

2.  **Configuraci√≥n Inicial (Opcional):**
    Si planea realizar modificaciones o compilaciones, puede ser necesario ejecutar el script de construcci√≥n:
    ```bash
    chmod +x build.sh
    ./build.sh
    ```

3.  **Ejecutar el Proyecto:**
    Abra el archivo `index.html` en su navegador web. Dado que el proyecto est√° dise√±ado para funcionar sin conexi√≥n, no se requiere un servidor web para la funcionalidad b√°sica.

    Para una experiencia m√°s robusta o si encuentra problemas de carga de recursos, puede servir los archivos usando un servidor web simple (por ejemplo, `python -m http.server` o `npx serve`):
    ```bash
    # Usando Python 3
    python3 -m http.server 8000
    # O si prefiere Node.js
    # npm install -g serve
    # serve
    ```
    Luego, acceda a `http://localhost:8000` en su navegador.

## üí° Uso

Una vez que el proyecto est√© cargado en su navegador, podr√° interactuar con el modelo de IA. La interfaz principal se encuentra en `index.html`.

### Ejemplo de Interacci√≥n

(Nota: La interacci√≥n espec√≠fica depender√° de la implementaci√≥n del modelo wllama y la interfaz de usuario desarrollada. A continuaci√≥n, se presenta un ejemplo gen√©rico.)

1.  Ingrese su consulta o prompt en el campo de texto proporcionado.
2.  Presione "Enviar" o la tecla Enter.
3.  La IA procesar√° su solicitud y generar√° una respuesta directamente en la interfaz.

```html
<!-- Ejemplo simplificado de la interfaz de usuario -->
<div class="chat-container">
    <div id="chat-output">
        <!-- Aqu√≠ se mostrar√°n los mensajes y respuestas -->
    </div>
    <div class="input-area">
        <input type="text" id="user-input" placeholder="Escribe tu pregunta aqu√≠...">
        <button id="send-button">Enviar</button>
    </div>
</div>
```

```javascript
// Ejemplo simplificado de la l√≥gica de interacci√≥n
document.getElementById('send-button').addEventListener('click', async () => {
    const userInput = document.getElementById('user-input').value;
    if (!userInput) return;

    // A√±adir mensaje del usuario al chat
    appendMessage('user', userInput);

    // L√≥gica para enviar la entrada al modelo wllama (implementaci√≥n a definir)
    // const aiResponse = await processWithWllama(userInput);

    // Simulaci√≥n de respuesta de la IA
    const aiResponse = `Respuesta simulada a: "${userInput}"`;
    appendMessage('ai', aiResponse);

    document.getElementById('user-input').value = '';
});

function appendMessage(sender, message) {
    const chatOutput = document.getElementById('chat-output');
    const messageElement = document.createElement('div');
    messageElement.classList.add('message', sender);
    messageElement.textContent = message;
    chatOutput.appendChild(messageElement);
    chatOutput.scrollTop = chatOutput.scrollHeight; // Scroll to bottom
}
```

## üìö Documentaci√≥n de la API

Este proyecto se enfoca en la ejecuci√≥n local de modelos de IA. La interacci√≥n principal se realiza a trav√©s de la interfaz de usuario y las funcionalidades expuestas por **wllama** en el lado del cliente (JavaScript y WebAssembly).

### Funciones Clave (Lado del Cliente)

*   `loadModel(modelPath)`: Carga un modelo wllama desde una ruta especificada.
*   `generateText(prompt, options)`: Genera texto basado en un prompt y opciones de configuraci√≥n (temperatura, longitud, etc.).
*   `preprocessInput(text)`: Prepara el texto de entrada para el modelo.
*   `postprocessOutput(text)`: Formatea la salida generada por el modelo.

(Nota: La documentaci√≥n detallada de la API de wllama se encuentra en la documentaci√≥n oficial de wllama, ya que este proyecto es un consumidor de dicha biblioteca.)

## üìÑ Licencia

Este proyecto no especifica una licencia. Por favor, consulte el repositorio para obtener informaci√≥n detallada sobre los derechos de uso y distribuci√≥n.

## ü§ù Contribuciones

Las contribuciones son bienvenidas. Por favor, consulte el archivo `CONTRIBUTING.md` (si existe) para conocer las directrices.

## ‚ö†Ô∏è Advertencia

Este proyecto utiliza WebAssembly (`.wasm`) para ejecutar modelos de IA en el navegador. El rendimiento y la compatibilidad pueden variar seg√∫n el navegador y el dispositivo. La eficiencia de la IA sin conexi√≥n depende en gran medida del tama√±o y la complejidad del modelo cargado.

---

<p align="center">
  <a href="https://readmeforge.app?utm_source=badge">
    <img src="https://readmeforge.app/badge.svg" alt="Made with ReadmeForge" height="20">
  </a>
</p>